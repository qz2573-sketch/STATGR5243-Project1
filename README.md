# STATGR5243-Project1: NYC Taxi Data Pipeline & Analysis

##  Project Overview
This project focuses on the end-to-end data science pipeline, ranging from raw data acquisition to exploratory data analysis (EDA) and feature engineering. [cite_start]The objective is to analyze trip records from the **NYC Taxi & Limousine Commission (TLC)** to derive meaningful insights regarding urban transportation patterns[cite: 3, 6].

The final deliverable includes a reproducible workflow that acquires data from the web, cleans inconsistencies, creates new features, and visualizes trends[cite: 17, 30].

##  Repository Structure
```text
├── data_raw/                  # Folder for raw, sampled datasets (generated by script)
│   ├── yellow_tripdata_2025_sample.csv
│   └── green_tripdata_2025_sample.csv
├── notebooks/                 # Jupyter/Quarto notebooks for analysis
│   └── analysis.ipynb         # Main EDA and Feature Engineering notebook
├── data_acquisition.py        # Python script for web scraping and sampling
├── Project_Report.pdf         # Final project report
├── README.md                  # Project documentation
└── requirements.txt           # Python dependencies
```

## Getting Started
1. Prerequisites
Ensure you have Python installed (3.8+ recommended). You will need the following libraries to handle web scraping and Parquet file processing:

Bash
pip install pandas requests beautifulsoup4 pyarrow fastparquet
2. Data Acquisition
To acquire the data, run the provided Python script. This script performs the following "Advanced" data selection tasks:


Web Scraping: Parses the NYC TLC Website to dynamically find download links.

Stratified Sampling: Downloads large Parquet files for July, August, September, and October 2025, and extracts a random sample of 200 records per month per vehicle type (Yellow and Green only). Each vehicle type will have 800 total records (4 months × 200 records/month) to ensure a manageable dataset size for analysis.

Command to run:

Bash
python data_acquisition.py
Output: This will generate two CSV files in the data_raw/ directory:
- yellow_tripdata_2025_sample.csv (800 rows: 200 per month × 4 months)
- green_tripdata_2025_sample.csv (800 rows: 200 per month × 4 months)

3. Running the Analysis
After generating the raw data, open notebooks/analysis.ipynb (or the equivalent R/Quarto file) to view the cleaning, preprocessing, and EDA steps.

## Methodology
Data Acquisition
Source: NYC Taxi & Limousine Commission (TLC) Trip Record Data.

Technique: Web Scraping via BeautifulSoup to extract .parquet file URLs.

Scope: 2025 Data (Months 7-10: July, August, September, October) for Yellow Taxi and Green Taxi. Each vehicle type includes 200 randomly sampled records per month, resulting in 800 records per type.

Data Cleaning & Handling Inconsistencies 

Type Conversion: Converting timestamp strings to datetime objects.


Missing Values: Imputing or removing records with null passenger_count or invalid location IDs.


Outliers: Filtering out trips with negative fares or unrealistic distances/durations.

Feature Engineering 

Temporal Features: Extracting "Hour of Day" and "Day of Week" to analyze traffic peaks.

Trip Duration: Calculating exact trip time from pickup and drop-off timestamps.

## Contributors
[Qixian Zhou (qz2573)]

[Kaicheng Li (kl3728)]

[Tiange Wang (tw3106)]

[Daisy Zhou (dz2590)]
